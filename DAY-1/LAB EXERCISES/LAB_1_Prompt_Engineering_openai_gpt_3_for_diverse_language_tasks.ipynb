{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RDcARFAvf-l4"
      },
      "id": "RDcARFAvf-l4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a7a9aa5d",
      "metadata": {
        "id": "a7a9aa5d"
      },
      "source": [
        "# Demo 1 - Customizing OpenAI GPT-3 for Diverse Language Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c7ab3c4-df7b-46a6-be0c-a70a45b3067d",
      "metadata": {
        "id": "6c7ab3c4-df7b-46a6-be0c-a70a45b3067d"
      },
      "source": [
        "## Introduction\n",
        "This lab focuses on demonstrating prompt engineering techniques using Azure OpenAI. Prompt engineering is a critical aspect of utilizing OpenAI models effectively, as it involves crafting prompts that generate desired outputs. In this lab, we will cover various scenarios, including QnA, summarizing text, classifying text, generating new product names, translation, parsing unstructured data, and translating natural language queries into SQL queries."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2160dc83-b832-470b-bbe5-0813e36636a5",
      "metadata": {
        "id": "2160dc83-b832-470b-bbe5-0813e36636a5"
      },
      "source": [
        "## Prerequisites\n",
        " - Ensure you have an Azure OpenAI account.\n",
        " - Install the required libraries: `openai` and `python-dotenv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bb96b08",
      "metadata": {
        "id": "4bb96b08"
      },
      "outputs": [],
      "source": [
        "%pip install openai==0.28.0 python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3d61902",
      "metadata": {
        "tags": [],
        "id": "e3d61902"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f79f5ad3",
      "metadata": {
        "tags": [],
        "id": "f79f5ad3"
      },
      "outputs": [],
      "source": [
        "sys.version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "457eb2a9",
      "metadata": {
        "tags": [],
        "id": "457eb2a9"
      },
      "outputs": [],
      "source": [
        "print(\"openai version =\", openai.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ab12be4-515a-4d70-bf1b-fe5e9dbb4523",
      "metadata": {
        "id": "5ab12be4-515a-4d70-bf1b-fe5e9dbb4523"
      },
      "source": [
        "Create an Azure OpenAI API key and set up your environment variables. Create a file named `azure.env` with the following content:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8242afa4",
      "metadata": {
        "tags": [],
        "id": "8242afa4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"CnlpRiTLzbJSEDTW3i1RcMa0GAkJiHqEe9Rse5mqVHUU0ZeVipvBJQQJ99BIACYeBjFXJ3w3AAAAACOGB28S\"\n",
        "os.environ[\"AZURE_OPENAI_API_BASE\"] = \"https://kedar.cognitiveservices.azure.com/\"\n",
        "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
        "os.environ[\"AZURE_OPENAI_MODEL_NAME\"] = \"gpt-35-turbo\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the environment variables (if set manually)\n",
        "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "api_base = os.getenv(\"AZURE_OPENAI_API_BASE\")\n",
        "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
        "model_name = os.getenv(\"AZURE_OPENAI_MODEL_NAME\")\n",
        "\n",
        "# Configure OpenAI for Azure\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_key = api_key\n",
        "openai.api_base = api_base\n",
        "openai.api_version = api_version\n",
        "\n",
        "model = \"gpt-35-turbo\""
      ],
      "metadata": {
        "id": "pJY3o96aIWca"
      },
      "id": "pJY3o96aIWca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "228ac1fa",
      "metadata": {
        "id": "228ac1fa"
      },
      "source": [
        "## Prompt Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "002a33a2",
      "metadata": {
        "id": "002a33a2"
      },
      "source": [
        "### 1. QnA\n",
        "In this section, we will use Azure OpenAI to answer questions.\n",
        "\n",
        "The primary goal of this section is to showcase how the model can understand and generate meaningful responses based on the given input."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Who are you?\n",
        "\"\"\"\n",
        "\n",
        "results = openai.ChatCompletion.create(\n",
        "    engine=model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature=0.7,  # You can adjust the temperature for more creative or focused outputs\n",
        "    max_tokens=50  # Increase max_tokens for longer responses\n",
        ")\n",
        "\n",
        "output_text = results[\"choices\"][0][\"message\"][\"content\"].strip(\"\\n\")\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "id": "8V-pTJeXl22D"
      },
      "id": "8V-pTJeXl22D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "448867c5-e9e7-4300-a55c-17545692192d",
      "metadata": {
        "id": "448867c5-e9e7-4300-a55c-17545692192d"
      },
      "source": [
        "### Explanation\n",
        "\n",
        "1.  **Prompt Definition:** The variable `prompt` contains the question provided to the model. In this case, the question is \"Who are you?\"\n",
        "    \n",
        "2.  **API Call:** The `openai.Completion.create` function is used to send a request to the OpenAI GPT-3.5 API. It takes parameters such as the engine (model), the prompt (question), temperature, and max_tokens (maximum number of tokens in the response).\n",
        "    \n",
        "3.  **Response Handling:** The API response is stored in the `results` variable, and the generated text is extracted from it using `results[\"choices\"][0][\"text\"]`. The `strip(\"\\n\")` function is used to remove leading and trailing newline characters.\n",
        "    \n",
        "4.  **Output Printing:** The final generated text is printed to the console using `print(results[\"choices\"][0][\"text\"].strip(\"\\n\"))`.\n",
        "    \n",
        "\n",
        "### Purpose\n",
        "\n",
        "-   **Demonstrating Question Understanding:** This section serves to illustrate how the OpenAI GPT-3.5 model can comprehend and respond appropriately to a given question. The model attempts to provide a coherent answer based on its understanding of the input prompt.\n",
        "    \n",
        "-   **Interaction with the Model:** It showcases the basic interaction pattern with the OpenAI API for question and answer tasks, emphasizing the simplicity of the API integration for such scenarios.\n",
        "    \n",
        "-   **User-Specific Applications:** This functionality is valuable for a range of applications, including chatbots, virtual assistants, and information retrieval systems where users can pose questions, and the system generates relevant responses.\n",
        "    \n",
        "-   **Flexibility in Questioning:** The model's ability to handle various types of questions and provide contextually relevant answers highlights its versatility in natural language understanding.\n",
        "    \n",
        "    \n",
        "\n",
        "> ### Note\n",
        ">\n",
        "> Depending on the nature of the prompt and the desired application, additional parameters such as temperature (controls randomness in the model's output) and max_tokens (limits the length of the response) can be adjusted to fine-tune the behavior of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb89a8de-f3c2-4f97-a5d6-3f5e58f201ba",
      "metadata": {
        "tags": [],
        "id": "fb89a8de-f3c2-4f97-a5d6-3f5e58f201ba"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Generate a YAML response for a product:\n",
        "- Product Name: iPhone 13\n",
        "- Price: $799\n",
        "- Availability: In stock\n",
        "- Features: 5G, OLED display, A15 chip\n",
        "\"\"\"\n",
        "\n",
        "results = openai.ChatCompletion.create(\n",
        "    engine=model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature=0.7,  # You can adjust the temperature for more creative or focused outputs\n",
        "    max_tokens=50  # Increase max_tokens for longer responses\n",
        ")\n",
        "\n",
        "output_text = results[\"choices\"][0][\"message\"][\"content\"].strip(\"\\n\")\n",
        "print(output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "824a70a9-5863-4650-a516-ed61df4e3496",
      "metadata": {
        "id": "824a70a9-5863-4650-a516-ed61df4e3496"
      },
      "source": [
        "Generate text in French based on a simple prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd361b3c-e1b7-4288-a067-f16a637f42b8",
      "metadata": {
        "tags": [],
        "id": "fd361b3c-e1b7-4288-a067-f16a637f42b8"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Bonjour\"\"\"\n",
        "\n",
        "results = openai.ChatCompletion.create(\n",
        "    engine=model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature=0.7,  # You can adjust the temperature for more creative or focused outputs\n",
        "    max_tokens=50  # Increase max_tokens for longer responses\n",
        ")\n",
        "\n",
        "output_text = results[\"choices\"][0][\"message\"][\"content\"].strip(\"\\n\")\n",
        "print(output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd8ca234-4f05-461b-9b68-09ed8c78fc75",
      "metadata": {
        "id": "bd8ca234-4f05-461b-9b68-09ed8c78fc75"
      },
      "source": [
        "### Explanation\n",
        "\n",
        "1.  **Prompt Definition:** The variable `prompt` contains a French greeting, \"Bonjour,\" which translates to \"Hello\" in English.\n",
        "    \n",
        "2.  **API Call:** The `openai.Completion.create` function is used to send a request to the OpenAI GPT-3.5 API. The parameters include:\n",
        "    \n",
        "    -   `engine`: Specifies the model to use (`text-davinci-003` in this case).\n",
        "    -   `prompt`: The input text provided to the model (`\"Bonjour\"` in French).\n",
        "    -   `temperature`: Controls the randomness of the model's output (set to `0` for deterministic responses).\n",
        "    -   `max_tokens`: Limits the length of the generated text to 800 tokens.\n",
        "3.  **Response Handling:** The API response is stored in the `results` variable. The generated text is then extracted using `results[\"choices\"][0][\"text\"]`. The `strip(\"\\n\")` function is used to remove leading and trailing newline characters.\n",
        "    \n",
        "4.  **Output Printing:** The final generated text is printed to the console using `print(results[\"choices\"][0][\"text\"].strip(\"\\n\"))`.\n",
        "    \n",
        "\n",
        "### Purpose\n",
        "\n",
        "-   **Language Translation:** This code snippet demonstrates the model's ability to understand and respond in French. It can be used for language translation tasks where the model takes input in one language and generates corresponding output in another language.\n",
        "    \n",
        "-   **Multilingual Capabilities:** GPT-3.5's multilingual capabilities allow it to handle prompts in various languages, showcasing its versatility in natural language understanding and generation.\n",
        "    \n",
        "-   **User Interface Localization:** In applications with multilingual user interfaces, this capability can be employed to dynamically generate responses in the user's preferred language.\n",
        "    \n",
        "\n",
        "> ### Note\n",
        ">\n",
        "> -   The choice of the `temperature` parameter influences the randomness of the model's responses. A lower temperature value (e.g., 0) produces more deterministic and focused output, while higher values  introduce more randomness.\n",
        ">\n",
        "> -   Adjustments to the `max_tokens` parameter can be made based on the desired length of the generated text. Setting an appropriate value prevents overly long responses.\n",
        ">     \n",
        "> -   This code illustrates how GPT-3.5 can seamlessly handle prompts in different languages, showcasing its potential in internationalization and localization contexts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "399df257",
      "metadata": {
        "tags": [],
        "id": "399df257"
      },
      "source": [
        "### 2. Summarize Text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60206f2a-d80c-4cf7-a573-a937469b860b",
      "metadata": {
        "id": "60206f2a-d80c-4cf7-a573-a937469b860b"
      },
      "source": [
        "Model's ability to summarize a given text into three short bullet points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a97dbc-8703-4d87-821a-348d2e47aadb",
      "metadata": {
        "tags": [],
        "id": "77a97dbc-8703-4d87-821a-348d2e47aadb"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Summarize below text in 3 short bullet points：\n",
        "\n",
        "            ###\n",
        "            A neutron star is the collapsed core of a massive supergiant star,\n",
        "            which had a total mass of between 10 and 25 solar masses,\n",
        "            possibly more if the star was especially metal-rich.\n",
        "            Neutron stars are the smallest and densest stellar objects,\n",
        "            excluding black holes and hypothetical white holes, quark stars,\n",
        "            and strange stars. Neutron stars have a radius on the order of\n",
        "            10 kilometres (6.2 mi) and a mass of about 1.4 solar masses.\n",
        "            They result from the supernova explosion of a massive star,\n",
        "            combined with gravitational collapse, that compresses the core\n",
        "            past white dwarf star density to that of atomic nuclei.\n",
        "            ###\n",
        "         \"\"\"\n",
        "\n",
        "results = openai.ChatCompletion.create(\n",
        "    engine=model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an astrolger assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature=0.7,  # You can adjust the temperature for more creative or focused outputs\n",
        "    max_tokens=800  # Increase max_tokens for longer responses\n",
        ")\n",
        "\n",
        "output_text = results[\"choices\"][0][\"message\"][\"content\"].strip(\"\\n\")\n",
        "print(output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24743ef3-5bd0-4e5a-894a-c56525661e83",
      "metadata": {
        "id": "24743ef3-5bd0-4e5a-894a-c56525661e83"
      },
      "source": [
        "### Explanation\n",
        "\n",
        "1.  **Prompt Definition:**\n",
        "    \n",
        "    -   The variable `prompt` contains a lengthy text describing neutron stars. The prompt instructs the model to summarize this text into three short bullet points.\n",
        "2.  **API Call:**\n",
        "    \n",
        "    -   The `openai.Completion.create` function is used to send a request to the OpenAI GPT-3.5 API.\n",
        "    -   The `engine` parameter specifies the GPT-3.5 engine or model to be used.\n",
        "    -   The `prompt` parameter provides the input text (description of a neutron star) to the model.\n",
        "    -   The `temperature` parameter is set to 0, indicating that the model's output should be deterministic and focused.\n",
        "    -   The `max_tokens` parameter limits the length of the generated text to 800 tokens.\n",
        "3.  **Response Handling:**\n",
        "    \n",
        "    -   The API response, including the generated text, is stored in the `results` variable.\n",
        "4.  **Output Printing:**\n",
        "    \n",
        "    -   The generated summary text is extracted from the API response using `results[\"choices\"][0][\"text\"].strip(\"\\n\")`.\n",
        "    -   The `strip(\"\\n\")` function removes leading and trailing newline characters.\n",
        "    -   The final result is printed to the console.\n",
        "\n",
        "### Purpose\n",
        "\n",
        "-   **Text Summarization:**\n",
        "    \n",
        "    -   The primary purpose is to showcase the model's ability to condense a longer piece of text into a concise summary, represented as three short bullet points.\n",
        "-   **Information Extraction:**\n",
        "    \n",
        "    -   It demonstrates how the model can extract key information from a given passage and present it in a structured format, which is particularly useful for distilling essential details from large bodies of text.\n",
        "-   **Automation of Summary Generation:**\n",
        "    \n",
        "    -   Developers can use this code as a foundation for automating the process of summarizing documents or articles, streamlining information extraction tasks.\n",
        "-   **Scalable Content Processing:**\n",
        "    \n",
        "    -   This functionality is valuable in scenarios where there is a need to process and summarize large volumes of textual information efficiently.\n",
        "\n",
        "> ### Note\n",
        ">\n",
        "> -   Developers can experiment with different prompts, text lengths, and temperature settings to fine-tune the summarization process based on their specific use case requirements.\n",
        ">     \n",
        "> -   This code snippet illustrates how the OpenAI GPT-3.5 model can be leveraged for content summarization, offering a glimpse into its capabilities in handling natural language understanding and generation  tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1988edc7-b4c8-47aa-a9ad-b9fdc7268235",
      "metadata": {
        "id": "1988edc7-b4c8-47aa-a9ad-b9fdc7268235"
      },
      "source": [
        "### Chinese Text Summarization\n",
        "\n",
        "The provided code snippet focuses on leveraging the OpenAI GPT-3.5 model to generate a brief summary of a Chinese text. The text in question describes the characteristics of a neutron star, including its mass, size, and the process by which it is formed. The goal is to obtain a concise summary represented in simplified Chinese."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e5c1b8b-ad07-4e17-af99-65c01baf6bc5",
      "metadata": {
        "tags": [],
        "id": "8e5c1b8b-ad07-4e17-af99-65c01baf6bc5"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"简要概括下面文字：\n",
        "\n",
        "            ###\n",
        "            中子星是一颗质量达10至25太阳质量（如果恒星特别富含金属可能更多）的超级巨星的坍缩核心。\n",
        "            中子星是最小最密集的恒星物体，除了黑洞和假想的白洞、夸克星和奇异星。\n",
        "            中子星的半径约为10公里（6.2英里），质量约为1.4太阳质量。\n",
        "            它们是由超级新星爆炸和引力坍缩共同产生的，使核心压缩到白矮星密度以上的原子核密度。\n",
        "            ###\n",
        "\n",
        "\n",
        "         \"\"\"\n",
        "\n",
        "results = openai.ChatCompletion.create(\n",
        "    engine=model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "        {\"role\": \"user\", \"content\": \"convert to english\"}\n",
        "    ],\n",
        "    temperature=0,  # You can adjust the temperature for more creative or focused outputs\n",
        "    max_tokens=800  # Increase max_tokens for longer responses\n",
        ")\n",
        "\n",
        "output_text = results[\"choices\"][0][\"message\"][\"content\"].strip(\"\\n\")\n",
        "print(output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2b37557-9e52-494b-b4eb-968f478fd1ad",
      "metadata": {
        "id": "d2b37557-9e52-494b-b4eb-968f478fd1ad"
      },
      "source": [
        "\n",
        "#### Explanation\n",
        "\n",
        "1.  **Prompt Definition:**\n",
        "    \n",
        "    -   The variable `prompt` contains a Chinese passage instructing the model to summarize the provided text.\n",
        "2.  **API Call:**\n",
        "    \n",
        "    -   The `openai.Completion.create` function is used to send a request to the OpenAI GPT-3.5 API.\n",
        "    -   The `engine` parameter specifies the GPT-3.5 engine or model to be used.\n",
        "    -   The `prompt` parameter provides the input text (description of a neutron star in Chinese) to the model.\n",
        "    -   The `temperature` parameter is set to 0, indicating that the model's output should be deterministic and focused.\n",
        "    -   The `max_tokens` parameter limits the length of the generated text to 800 tokens.\n",
        "3.  **Response Handling:**\n",
        "    \n",
        "    -   The API response, including the generated summary text, is stored in the `results` variable.\n",
        "4.  **Output Printing:**\n",
        "    \n",
        "    -   The generated summary text is extracted from the API response using `results[\"choices\"][0][\"text\"].strip(\"\\n\")`.\n",
        "    -   The `strip(\"\\n\")` function removes leading and trailing newline characters.\n",
        "    -   The final result is printed to the console.\n",
        "\n",
        "#### Purpose\n",
        "\n",
        "-   **Cross-Language Summarization:**\n",
        "    \n",
        "    -   This code exemplifies the OpenAI GPT-3.5 model's capability to summarize information in a language other than English, showcasing its multilingual text processing capabilities.\n",
        "-   **Automated Content Summarization:**\n",
        "    \n",
        "    -   It illustrates how the model can be used to automate the process of summarizing content in Chinese, providing a concise overview of a given text.\n",
        "-   **Language Understanding and Generation:**\n",
        "    \n",
        "    -   The code demonstrates the model's proficiency in understanding and generating text in a non-English language, extending its utility to a global context.\n",
        "\n",
        "#### Note\n",
        "\n",
        "-   Developers can adapt this code for various applications, such as building multilingual summarization tools or integrating cross-language capabilities into their natural language processing workflows.\n",
        "    \n",
        "-   Adjustments to the prompt, temperature, and other parameters can be made based on specific use case requirements and language nuances."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7814896f",
      "metadata": {
        "id": "7814896f"
      },
      "source": [
        "### 3. Classify Text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f79f567f-ac44-4cec-bbdb-585522b770d8",
      "metadata": {
        "id": "f79f567f-ac44-4cec-bbdb-585522b770d8"
      },
      "source": [
        "`Classify Text` refers to the task of assigning predefined categories or labels to a given piece of text based on its content and context. The goal is to automatically categorize the text into one or more predefined classes, making it easier to organize, search, and analyze large volumes of textual data. This task falls under the broader field of natural language processing (NLP) and is commonly used in various applications, including sentiment analysis, topic categorization, and content filtering.\n",
        "\n",
        "The process typically involves training a machine learning model on a labeled dataset where each text sample is associated with its corresponding category. The model learns patterns and features from the training data, allowing it to generalize and classify new, unseen text accurately.\n",
        "\n",
        "In the context of the provided code snippet, \"Classify Text\" specifically refers to instructing the OpenAI GPT-3.5 model to categorize a news article into one of the specified categories: Tech, Politics, Sport, or Entertainment. The model generates a response that represents its prediction of the most suitable category based on its understanding of the given news article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bcd125e-407b-4d97-bd92-e4e8dc3d4e5d",
      "metadata": {
        "tags": [],
        "id": "9bcd125e-407b-4d97-bd92-e4e8dc3d4e5d"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Classify the following news article into 1 of the following categories:\n",
        "            [Tech, Politics, Sport, Entertainment]\n",
        "\n",
        "            ###\n",
        "            Donna Steffensen Is Cooking Up a New Kind of Perfection.\n",
        "            The Internet’s most beloved cooking guru has a buzzy new book and\n",
        "            a fresh new perspective: Entertainment\n",
        "\n",
        "            Donna Steffensen Is Cooking Up a New Kind of Perfection.\n",
        "            The Internet’s most beloved cooking guru has a buzzy new book and\n",
        "            a fresh new perspective: Politics\n",
        "\n",
        "            Maya's heart pounded as the final buzzer blared.\n",
        "            Sweat stung her eyes, but a smile stretched across her face.\n",
        "            The scoreboard confirmed it: victory by one point.\n",
        "            Exhausted but elated, Maya high-fived her teammates,\n",
        "            the cheers of the crowd a sweet reward.:\n",
        "\n",
        "            ###\n",
        "         \"\"\"\n",
        "\n",
        "results = openai.ChatCompletion.create(\n",
        "    engine=model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature=0.7,  # You can adjust the temperature for more creative or focused outputs\n",
        "    max_tokens=800  # Increase max_tokens for longer responses\n",
        ")\n",
        "\n",
        "output_text = results[\"choices\"][0][\"message\"][\"content\"].strip(\"\\n\")\n",
        "print(output_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "3M6bvFXomL5L"
      },
      "id": "3M6bvFXomL5L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e6109264-5d7f-4e93-acb8-365e4852e38e",
      "metadata": {
        "id": "e6109264-5d7f-4e93-acb8-365e4852e38e"
      },
      "source": [
        "#### Description\n",
        "\n",
        "1.  **Prompt Definition:**\n",
        "    \n",
        "    -   The `prompt` variable sets the instructions for the model. It instructs the model to classify the provided news article into one of the predefined categories: Tech, Politics, Sport, or Entertainment.\n",
        "2.  **News Article:**\n",
        "    \n",
        "    -   The provided news article is about Donna Steffensen, described as a cooking guru with a new book and a fresh perspective.\n",
        "3.  **API Call:**\n",
        "    \n",
        "    -   The `openai.Completion.create` function sends a request to the OpenAI GPT-3.5 API for text generation.\n",
        "    -   The `engine` parameter specifies the GPT-3.5 engine or model to be used.\n",
        "    -   The `prompt` parameter supplies the input text (news article and classification instruction) to the model.\n",
        "    -   The `temperature` parameter is set to 0, indicating a deterministic response without randomness.\n",
        "    -   The `max_tokens` parameter limits the length of the generated text to 800 tokens.\n",
        "4.  **Response Handling:**\n",
        "    \n",
        "    -   The API response, including the generated text (classification result), is stored in the `results` variable.\n",
        "5.  **Output Printing:**\n",
        "    \n",
        "    -   The generated classification result is extracted from the API response using `results[\"choices\"][0][\"text\"].strip(\"\\n\")`.\n",
        "    -   The `strip(\"\\n\")` function removes leading and trailing newline characters.\n",
        "    -   The final result is printed to the console.\n",
        "\n",
        "#### Purpose\n",
        "\n",
        "-   **Text Classification:**\n",
        "    \n",
        "    -   The code demonstrates the use of the OpenAI GPT-3.5 model for text classification. The model categorizes a given news article into one of the specified classes (Tech, Politics, Sport, or Entertainment).\n",
        "-   **Automated Categorization:**\n",
        "    \n",
        "    -   It showcases the potential for automating the categorization of news articles based on their content, providing a quick and efficient way to organize information.\n",
        "-   **Understanding Context:**\n",
        "    \n",
        "    -   The model's ability to comprehend the context of the news article and assign it to a relevant category highlights its proficiency in understanding natural language and context.\n",
        "\n",
        "#### Note\n",
        "\n",
        "-   Developers can customize the prompt and categories based on their specific use case, expanding the application to various domains that involve text classification.\n",
        "    \n",
        "-   Fine-tuning the parameters, such as temperature, can be explored to influence the randomness and creativity of the generated classification result."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7127f95b",
      "metadata": {
        "id": "7127f95b"
      },
      "source": [
        "### 4. Generate New Product Name\n",
        "\n",
        "It automatically create a unique product name based on specified information. With a product description of a \"home milkshake maker\" and seed words like \"fast, healthy, compact,\" the model generates a fitting name inspired by provided examples. This showcases the capability of the model in creative product naming, offering a streamlined and automated approach for inventing distinctive names for various products."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68e7324a-e559-42f6-90ce-58f98a92a8b2",
      "metadata": {
        "tags": [],
        "id": "68e7324a-e559-42f6-90ce-58f98a92a8b2"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"Generate new product name based on the following information：\n",
        "\n",
        "            ###\n",
        "            Product description: A home milkshake maker\n",
        "            Seed words: fast, healthy, compact\n",
        "            Product names: HomeShaker, Fit Shaker, QuickShake, Shake Maker\n",
        "            ###\n",
        "         \"\"\"\n",
        "\n",
        "results = openai.ChatCompletion.create(\n",
        "    engine=model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature=0.7,  # You can adjust the temperature for more creative or focused outputs\n",
        "    max_tokens=800  # Increase max_tokens for longer responses\n",
        ")\n",
        "\n",
        "output_text = results[\"choices\"][0][\"message\"][\"content\"].strip(\"\\n\")\n",
        "print(output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b40e666",
      "metadata": {
        "id": "4b40e666"
      },
      "source": [
        "### 5. Translation\n",
        "\n",
        "In \"Translation\" you translate a Chinese poem into English. The provided poem describes the scenery of mountains, the flow of the Yellow River into the sea, and the desire to explore distant landscapes. The model generates an English translation, showcasing its ability to understand and convert text between different languages. The resulting translation is then printed to the console, demonstrating the model's proficiency in cross-language tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df4f0220",
      "metadata": {
        "tags": [],
        "id": "df4f0220"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"请用英语翻译下面这首诗歌：\n",
        "\n",
        "            ###\n",
        "            白日依山尽，黄河入海流。\n",
        "            欲穷千里目，更上一层楼。\n",
        "            ###\n",
        "         \"\"\"\n",
        "\n",
        "results = openai.ChatCompletion.create(\n",
        "    engine=model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature=0.7,  # You can adjust the temperature for more creative or focused outputs\n",
        "    max_tokens=800  # Increase max_tokens for longer responses\n",
        ")\n",
        "\n",
        "output_text = results[\"choices\"][0][\"message\"][\"content\"].strip(\"\\n\")\n",
        "print(output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d783da05",
      "metadata": {
        "id": "d783da05"
      },
      "source": [
        "### 6. Parse Unstructured Data\n",
        "\n",
        "In \"Parse Unstructured Data\" you process information about fruits on the fictional planet Goocrux. Descriptions of various fruits, such as neoskizzles, loheckles, pounits, loopnovas, and glowls, are provided. The code instructs the model to create a structured table summarizing these fruits, including details like color and flavor. The resulting table is generated by the model and printed to the console, showcasing the model's ability to organize and present information in a tabular format based on unstructured input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d38ed82e",
      "metadata": {
        "tags": [],
        "id": "d38ed82e"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"There are many fruits that were found on the recently discovered planet Goocrux.\n",
        "            There are neoskizzles that grow there, which are purple and taste like candy.\n",
        "            There are also loheckles, which are a grayish blue fruit and are very tart, a\n",
        "            little bit like a lemon. Pounits are a bright green color and are more savory\n",
        "            than sweet. There are also plenty of loopnovas which are a neon pink flavor and\n",
        "            taste like cotton candy. Finally, there are fruits called glowls, which have a very\n",
        "            sour and bitter taste which is acidic and caustic, and a pale orange tinge to them.\n",
        "\n",
        "            ###\n",
        "            Please make a json summarizing the fruits from Goocrux\n",
        "            { \"Fruit\":\n",
        "              \"Color\":\n",
        "              \"Flavor\":}\n",
        "            ###\n",
        "         \"\"\"\n",
        "\n",
        "results = openai.ChatCompletion.create(\n",
        "    engine=model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature=0.7,  # You can adjust the temperature for more creative or focused outputs\n",
        "    max_tokens=800  # Increase max_tokens for longer responses\n",
        ")\n",
        "\n",
        "output_text = results[\"choices\"][0][\"message\"][\"content\"].strip(\"\\n\")\n",
        "print(output_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "035b8a78",
      "metadata": {
        "id": "035b8a78"
      },
      "source": [
        "### 7. NLP to SQL\n",
        "\n",
        "In \"NLP to SQL\" you convert natural language instructions into a SQL query. The provided prompt defines three PostgreSQL tables (Employee, Department, and Salary_Payments) with their respective properties. The code then instructs the model to generate a SQL query for listing the names of departments that employed more than 10 employees in the last 3 months.\n",
        "\n",
        "The resulting SQL query is generated by the model and printed to the console, showcasing the model's ability to understand and convert natural language queries into structured SQL statements for database operations. This code provides an example of how natural language processing can be applied to automate the generation of SQL queries based on human-readable instructions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80feffef",
      "metadata": {
        "tags": [],
        "id": "80feffef"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"### Postgres SQL tables, with their properties:\n",
        "            #\n",
        "            # Employee(id, name, department_id)\n",
        "            # Department(id, name, address)\n",
        "            # Salary_Payments(id, employee_id, amount, date)\n",
        "            #\n",
        "\n",
        "            ### A query to list the names of the departments\n",
        "                which employed more than 10 employees in the last 3 months\n",
        "            ###\n",
        "         \"\"\"\n",
        "\n",
        "results = openai.ChatCompletion.create(\n",
        "    engine=model,\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt}\n",
        "    ],\n",
        "    temperature=0.7,  # You can adjust the temperature for more creative or focused outputs\n",
        "    max_tokens=800  # Increase max_tokens for longer responses\n",
        ")\n",
        "\n",
        "output_text = results[\"choices\"][0][\"message\"][\"content\"].strip(\"\\n\")\n",
        "print(output_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df20daa8",
      "metadata": {
        "id": "df20daa8"
      },
      "outputs": [],
      "source": [
        "# @title Default title text\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}